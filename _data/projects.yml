# TODO: add sanskrit-ocr
- title: Tasq Anomaly Detection
  type: Professional Project
  tools_and_technologies: Python, Pandas, NumPy, SciPy, DTW Distance, Typescript, Node.js, Vue.js, Vuetify, Express.js, REST, AWS SDK
  domains: Big Data, Data Analysis, Machine Learning, Web Designing and Development
  summary: >
    Tasq is an AI based approach to detect anomalies (unusual/unexpected patterns) that occur in
    the working process of oil/gas wells. It requires an infrastructure to work which consists
    of multiple sensors attached to the wells and transmitting live data to the cloud.
    This system helps in monitoring the whole process end-to-end based on the data received from sensors.
    Periodically trained clustering models (unsupervised), as getting labelled data of this
    huge amount of timeseries data (sensor readings) is a cumbersome process.
    Conducted analysis of data and tried different tweaks which proved useful for improving accuracy.
    Integrated users' feedback into the core detection algorithm with which
    we got a boost in accuracy of about 10%.
    Also, built an easy to use dashboard for this large ecosystem which is capable of
    combining data from all over from this distributed system and display it at a single place,
    and let users (analysts) interact with it.
  start_date: October 2020
  end_date: January 2021

- title: Smart Kitchen
  type: Professional Project
  tools_and_technologies: Python, MySQL, Docker, OpenCV, TensorFlow
  domains: Deep Learning, Image Processing, Operating Systems
  summary: >
    In this COVID-19 era, utmost care is needed to be taken in restaurants when cooking food,
    as cooked food is impossible to sanitize. We built a system which can ensure that maximum care
    has been taken to prepare food in kitchens of restaurants, hotels, take-aways.
    Integrated a two stage DL processing pipeline in order to detect if a chef/cook has broken a rule
    and raise alerts on violation. The system is built to detect if chefs have worn proper attires
    such as gloves, mask, cap and apron when working in the kitchen using FasterRCNN trained on our manually annotated dataset.
    This system also utilizes the power of parallel processing for making it work in real time.
  start_date: July 2020
  end_date: October 2020

- title: H+Tree Database Storage Engine (HDB)
  type: Professional Project
  tools_and_technologies: Python, C++, MariaDB, InnoDB, CodeStream, CMake
  domains: Data Structures and Algorithms
  summary: >
    The main aim is to replace the B+Tree data structure with H+Tree in large codebase of MariaDB (in C++).
    The challenges are understanding and modifying high number of tightly coupled modules of in-memory and on-disk data structures
    which are very less documented. Properly planned the project to maintain the speed of development. 
    Guide the team and monitor the development and testing of a MVCC based SQL server (in Python as a POC).
    Wrote custom unittests and modified the CMake build system to avoid unnecessary errors.
    Explaining the approach and issues to the team so that they can work with a goal oriented mindset.
    The main issues which we faced was that some logic that was used in B+Tree would not work the same in H+Tree.
    Conduct periodic meetings with the team to understand such issues and provide possible solutions.
  start_date: December 2019
  end_date: Present

- title: Object Detection Meets Knowledge Graphs
  type: Research Paper Implementation (Professional)
  tools_and_technologies: Python, NumPy, TensorFlow
  domains: Deep Learning, Data Science
  summary: >
    In this paper, the authors propose an idea to integrate external knowledge into SOTA object detection algorithms, 
    with a view to re-optimize the output of these models. The main aim is to use semantic knowledge of objects to improve detection. 
    A knowledge graph (matrix representation) was built from a large text corpus which was processed to extract similarity of objects. 
    Then this matrix was used to refine the output of SOTA models through an iterative algorithm.
  start_date: February 2020
  end_date: March 2020

- title: Vergesource
  type: Professional Project
  tools_and_technologies: Python, Regex, Java, Django, BeautifulSoup, Grobid
  domains: Data Structures and Algorithms, Deep Learning, Data Science, Web Development
  summary: >
    It is a research paper builder, a simple web based UI provided to users who are willing to write scientific papers
    in various different formats like APA, IEEE, etc.
    Key features include parsing already written paper, writing a new one, recommending citations and linking them,
    and providing the output in various different paper formats.
    Used Grobid server to extract data and it's metadata from a paper's PDF and
    used regex with other data processing techniques to clean, improve and modify the extracted data.
  start_date: December 2019
  end_date: January 2020

- title: H+Tree V1
  type: Research based Professional Project
  tools_and_technologies: Python, C++, InnoDB, B+Tree
  domains: Data Structures and Algorithms, Machine Learning, Data Analytics
  summary: >
    H+Tree is an efficient way of storing data in databases.
    High speed and less memory (as well as storage) requirements is the main aim of this data structure.
    Currently, the storage engine being used widely in database servers is InnoDB which used B+Tree as it's data structure to store data internally.
    H+Tree aims at optimizing the internal B+Tree module of database storage systems 
    in order to have a new storage system which is faster and more storage efficient.
    Researched for the possible places of optimizations in B+Tree by understanding it's working with the user's data.
    Developed few different variations of H+Tree and took benchmarks for comparison with B+Tree.
    Analysed the tree structure with stored data to find possible issues in those different variations.
    Developed and tested a 2x faster algorithm in Python and helped the team with implementation and debugging
    of the same algorithm in C++.
  start_date: May 2019
  end_date: December 2019

- title: Artificial Neural Networks and Interpretability
  type: Professional Mini Project
  tools_and_technologies: Python, TensorFlow, Scikit-Learn, Matplotlib, Pandas, NumPy
  domains: Machine Learning, Deep Learning, Data Analysis
  summary: >
    This project was meant to be a part of training lab for others and hence visualizing the trained network on datasets was important.
    End-to-end task was to interpret what ANNs learn.
    Wrote neural networks from scratch using NumPy and implemented backpropagation using a simple gradient descent optimizer.
    Built and trained different models and visualized their output via training as well as by manually setting up the weights.
    Trained various ML models like decision trees and random forests, and visualized their feature importance graphs.
    Used a decision tree trained on the output of an ANN in order to interpret what the ANN learnt,
    which features are important to the ANN, how much did we advantage by using an ANN over a simple ML model.
  start_date: November 2019
  end_date: November 2019

- title: <a href="https://github.com/ParikhKadam/mnist-experiments/blob/master/experiment1.ipynb">Location Invariant Feature Learning in CNN</a>
  type: Experiment out of Curiosity
  tools_and_technologies: Python, Keras, NumPy, Matplotlib
  domains: Data Processing, Computer Vision, Deep Learning
  summary: >
    Most of Deep Learning Engineers believe that if we train a CNN on a dataset in which the objects
    (important patterns) are always in a fixed region in the training set,
    then the model will perform poorly on images which contains objects in the other regions,
    as it never saw objects in these regions during training. But that's not true.
    CNNs are capable of detecting patterns in any part of the image and hence, it is possible.
    So, I did a small experiment to show that. Understanding this, one can tell why pooling layers are important!
  start_date: August 2019
  end_date: August 2019

- title: Palpebra Air
  type: Professional Project
  tools_and_technologies: Python, TensorFlow, OpenCV, MySQL, FLask
  domains: Computer Vision, Deep Learning, Web Development
  summary: >
    Built a graphic search engine using object detection.
    The system is built for content creators. One daily task of content creators is looking at the variety of samples
    to gain an understanding and uniqueness of their task. It is a graphic search engine where the user can search for
    various entities and the system will return all the images/videos that contain these entities.
    The system works in two phases. At first, the user provides several images/videos for processing.
    Then the system processes this data and stores the extracted information in database.
    Now, the user can search for entities.
  start_date: March 2019
  end_date: April 2019

- title: <a href="https://github.com/ParikhKadam/bidaf-keras">Machine Comprehension model for Question Answering using Deep Learning</a>
  type: Research Paper Implementation (OpenSource)
  tools_and_technologies: Python, Keras, NumPy, NLTK
  domains: Natural Language Processing, Natural Language Understanding, Deep Learning
  summary: >
    This is my final year project in BE CSE.
    Implemented <a href="https://arxiv.org/abs/1611.01603">Bidirectional Attention Flow for Machine Comprehension</a>
    in Keras from scratch. Machine comprehension is a task of answering a question from a given paragraph.
    It uses attention mechanism to focus on a small portion of the paragraph and summarize it with a fixed-size vector,
    couple attentions temporally, and output probability spans for answer start and end.
    Learnt all the concepts of Machine Learning and Deep Learning with some expertise in Natural Language Processing.
    Dissected the paper for thoroughly understanding the mathematics behind it's intuitive logic.
    Learnt Keras with a hands-on and implemented the basic model.
    Added several features which provides flexibility for others using this project, and all this was done within just one year.
  start_date: November 2018
  end_date: April 2019

- title: Adaptive Content Scraping from Google 
  type: Mini Project
  tools_and_technologies: Python, BeautifulSoup, NLTK
  domains: Web Scraping
  summary: >
    Built a scraping application which can scrape Google search results for a particular query provided by the user.
    It is able to extract title, url and text from search results. Not only that, but I also added a simple algorithm
    which is capable of extracting data even if the structure of webpage changes in future.
    Hence, the application is able to adapt to the change in the result's page structure.
  start_date: May 2019
  end_date: May 2019

- title: Sentiment analysis on IMDB reviews dataset
  type: Mini Project
  tools_and_technologies: Python, Keras
  domains: Natural Language Processing, Deep Learning
  summary: >
    The task included developing neural network architectures based on supervised learning
    which could learn from the input data to predict whether the review was positive or negative.
    I developed two different architectures and compared them on the dataset.
    One was a simple Deep Dense neural network while the other was a RNN architecture. Obviously, the RNN performed better.
  start_date: October 2018
  end_date: October 2018

- title: ITI Job Portal
  type: Team Project
  tools_and_technologies: Core PHP, HTML, CSS, Bootstrap, Javascript, MariaDB
  domains: Web Designing and Development, Database Design and Management
  summary: >
    A website similar to <a href="https://linkedin.com">LinkedIn</a> but specialized for ITI students and professionals.
    We, as a team, developed a job portal using core PHP, Bootstrap and JQuery. The project was aimed to help us learn the
    concepts of web development and security i.e. data transmission, session management, password hashing and salt,
    implementing access control mechanisms and such.
  start_date: November 2017
  end_date: March 2018
